{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc40c48b-0c95-4757-a067-563cfccd51a5",
   "metadata": {
    "id": "dc40c48b-0c95-4757-a067-563cfccd51a5",
    "tags": []
   },
   "source": [
    "# Invoke FM (Amazon Titan Model)  for Generating Text (Email Responses) Using Bedrock API\n",
    "\n",
    "How to use Large Language Model (LLM) to generate an email response to a customer who provided negative feedback on the quality of customer service they received from the support engineer. In this notebook, We  generate an email with a thank you note based on the customer's previous email. We use the Amazon Titan model using the **Amazon Bedrock API with Boto3 client**.\n",
    "\n",
    "The prompt used in this task is called a zero-shot prompt. In a zero-shot prompt, you describe the task or desired output to the language model in plain language. The model then uses its pre-trained knowledge and capabilities to generate a response or complete the task based solely on the provided prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a413e2-3c34-4073-9000-d8556537bb6a",
   "metadata": {
    "id": "c9a413e2-3c34-4073-9000-d8556537bb6a"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8e2797f9",
   "metadata": {
    "id": "8e2797f9"
   },
   "source": [
    "## 1. Environment setup\n",
    "\n",
    "In this task, you set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "776fd083",
   "metadata": {
    "id": "776fd083",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries to work with AWS, files, and environment variables\n",
    "import json  # For working with JSON data\n",
    "import os  # For interacting with the operating system (e.g., file paths)\n",
    "import sys  # For modifying system paths\n",
    "\n",
    "# Importing Boto3 (AWS SDK for Python) and botocore (handles low-level AWS interactions)\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Adding the parent directory to the system path so we can import files from there\n",
    "module_path = \"..\"  # This points to the parent directory\n",
    "sys.path.append(os.path.abspath(module_path))  # Adds the parent directory to the search path for modules\n",
    "\n",
    "# Create a connection to Amazon Bedrock service using the Boto3 client\n",
    "# 'bedrock-runtime' specifies the service we want to connect to\n",
    "# The region_name is fetched from the environment variable AWS_DEFAULT_REGION\n",
    "bedrock_client = boto3.client(\n",
    "    'bedrock-runtime',  # Service name: Bedrock\n",
    "    region_name=os.environ.get(\"AWS_DEFAULT_REGION\", None)  # AWS region, if available from environment settings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HGyiqd5_jpXL",
   "metadata": {
    "id": "HGyiqd5_jpXL"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "#### **Importing Libraries:**\n",
    "\n",
    "- **json**: This library is used to handle **JSON data**, such as parsing or creating JSON files.\n",
    "- **os**: This module helps interact with the **operating system**, for tasks like accessing file paths or environment variables.\n",
    "- **sys**: Used for modifying **system paths**, allowing the script to access modules from different locations.\n",
    "\n",
    "#### **Boto3 and Botocore:**\n",
    "\n",
    "- **boto3**: This is the **AWS SDK for Python**, which allows you to interact with **AWS services** like Amazon Bedrock.\n",
    "- **botocore**: This library handles the **low-level interaction** with AWS services, managing tasks such as making requests and receiving responses.\n",
    "\n",
    "#### **Modifying System Path:**\n",
    "\n",
    "- The code adds the **parent directory** (`\"..\"`) to the **system path**, enabling the script to access files or modules from that directory.\n",
    "- **`sys.path.append()`** ensures that Python can find and import the necessary files from the parent folder.\n",
    "\n",
    "#### **Connecting to Amazon Bedrock:**\n",
    "\n",
    "- **`boto3.client()`**: This creates a **connection** to Amazon Bedrock using the service name `'bedrock-runtime'`.\n",
    "- **`region_name`** is fetched from the environment variable **`AWS_DEFAULT_REGION`**, which defines the region to connect to (e.g., `us-east-1`). If not set, it defaults to `None`.\n",
    "- This step **establishes communication** with the Amazon Bedrock service, allowing you to interact with its **AI models** for tasks like **text generation** or other language-related tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f634211-3de1-4390-8c3f-367af5554c39",
   "metadata": {
    "id": "4f634211-3de1-4390-8c3f-367af5554c39"
   },
   "source": [
    "## 2. Generate text\n",
    "\n",
    "In this task, you prepare an input for the Amazon Bedrock service to generate an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45ee2bae-6415-4dba-af98-a19028305c98",
   "metadata": {
    "id": "45ee2bae-6415-4dba-af98-a19028305c98",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the prompt to be sent to the AI model for generating an email\n",
    "\n",
    "# This is the instruction that tells the model what to do\n",
    "\n",
    "prompt_data = \"\"\"\n",
    "Command: Write an email from Bob, Customer Service Manager, AnyCompany to the customer \"John Doe\"\n",
    "who provided negative feedback on the service provided by our customer support\n",
    "engineer\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aFMbeyGkwab",
   "metadata": {
    "id": "4aFMbeyGkwab"
   },
   "source": [
    "This code is preparing an instruction (known as a prompt) that will be sent to an AI model to generate an email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af670eb-ad02-40df-a19c-3ed835fac8d9",
   "metadata": {
    "id": "8af670eb-ad02-40df-a19c-3ed835fac8d9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare the data to send to the AI model in JSON format\n",
    "body = json.dumps({\n",
    "    \"inputText\": prompt_data,  # The prompt we created earlier (email request)\n",
    "    \"textGenerationConfig\": {  # Settings for generating the text\n",
    "        \"maxTokenCount\": 8192,  # Max length of the response\n",
    "        \"stopSequences\": [],  # No specific stop point\n",
    "        \"temperature\": 0,  # Predictable, less random response\n",
    "        \"topP\": 0.9  # Controls how varied the response can be\n",
    "    }\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CDp763Cwl_GM",
   "metadata": {
    "id": "CDp763Cwl_GM"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This section of the code prepares the **data** to send to the AI model for generating text. The data is formatted in **JSON** format and contains the prompt and configuration settings.\n",
    "\n",
    "1. **`inputText`**:\n",
    "   - This is the **prompt** that we created earlier (the request to generate an email). It contains the task for the AI model to perform — in this case, writing an email in response to customer feedback.\n",
    "\n",
    "2. **`textGenerationConfig`**:\n",
    "   - This defines the **settings** for how the AI should generate the text. It includes the following parameters:\n",
    "   \n",
    "   - **`maxTokenCount`**: This sets the maximum length of the generated text, in terms of tokens. A **token** can be a word or part of a word, so setting it to **8192** allows the model to generate a reasonably long response.\n",
    "   \n",
    "   - **`stopSequences`**: This defines any specific **stop points** where the AI should halt generating text. In this case, it's left as an empty list (`[]`), meaning the AI will not stop until it reaches the **end of the prompt** or maximum token count.\n",
    "   \n",
    "   - **`temperature`**: This controls the **randomness** of the generated response. A value of `0` means the response will be **predictable** and **deterministic** (i.e., the model will choose the most likely next token). Higher values like `0.7` would make the response more **random**.\n",
    "   \n",
    "   - **`topP`**: This parameter controls how **varied** the model's response can be. A value of `0.9` means the model will consider the top **90% of probable next tokens** for generating the response. This helps the model produce more natural and creative outputs while avoiding overly repetitive or irrelevant text.\n",
    "\n",
    "### Purpose:\n",
    "The code creates a structured **request** to send to the AI model. The `inputText` provides the instructions for the task (writing the email), while the `textGenerationConfig` ensures that the model generates the response with the desired length, creativity, and predictability. This setup helps guide the AI model in generating the response according to specific requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088cf6bf-dd73-4710-a0cc-6c11d220c431",
   "metadata": {
    "id": "088cf6bf-dd73-4710-a0cc-6c11d220c431"
   },
   "source": [
    "## 3. Invoke the Amazon Titan Large language model\n",
    "\n",
    "In this task, you explore how the model generates an output based on the prompt created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379498f2",
   "metadata": {
    "id": "379498f2"
   },
   "source": [
    "### Complete Output Generation\n",
    "\n",
    "This email is generated using the Amazon Titan model by understanding the input request and utilizing its inherent understanding of different modalities. The request to the API is synchronous and waits for the entire output to be generated by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecaceef1-0f7f-4ae5-8007-ff7c25335251",
   "metadata": {
    "id": "ecaceef1-0f7f-4ae5-8007-ff7c25335251",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the model ID and request headers\n",
    "modelId = 'amazon.titan-text-express-v1'  # Specify the model version to use\n",
    "accept = 'application/json'  # The expected response format\n",
    "contentType = 'application/json'  # The format of the request body\n",
    "outputText = \"\\n\"  # Initialize an empty string to store the response\n",
    "\n",
    "try:\n",
    "    # Invoke the AI model using the Bedrock client\n",
    "    response = bedrock_client.invoke_model(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "\n",
    "    # Read and parse the response from the model\n",
    "    response_body = json.loads(response.get('body').read())\n",
    "\n",
    "    # Extract the generated output text from the response\n",
    "    outputText = response_body.get('results')[0].get('outputText')\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Handle specific errors returned by AWS\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        # If access is denied, print an error message with troubleshooting links\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Message']}\\n\"\n",
    "              \"To troubleshoot this issue, please refer to the following resources.\\n\"\n",
    "              \"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\n\"\n",
    "              \"https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "    else:\n",
    "        # For any other errors, re-raise the error\n",
    "        raise error\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9QU6ddzmbsg",
   "metadata": {
    "id": "c9QU6ddzmbsg"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This code interacts with **Amazon Bedrock** to generate text from an AI model, using the **boto3 client** to send requests and handle responses.\n",
    "\n",
    "1. **Setting Model ID and Request Headers**:\n",
    "   - **`modelId`**: Specifies the version of the AI model to use (in this case, `'amazon.titan-text-express-v1'`).\n",
    "   - **`accept`**: Defines the format of the expected response, which is **JSON**.\n",
    "   - **`contentType`**: Specifies that the request body will be in **JSON** format.\n",
    "   - **`outputText`**: Initializes an empty string that will later store the generated response.\n",
    "\n",
    "2. **Invoking the AI Model**:\n",
    "   - **`bedrock_client.invoke_model()`** sends the request to the **Amazon Bedrock service** using the specified model ID and the provided data (`body`).\n",
    "   - The response is expected in **JSON** format, as specified by the **`accept`** and **`contentType`** headers.\n",
    "\n",
    "3. **Reading and Parsing the Response**:\n",
    "   - The response from the AI model is received in **JSON** format and is parsed using **`json.loads()`** to convert the response into a Python dictionary.\n",
    "\n",
    "4. **Extracting the Generated Text**:\n",
    "   - The **`outputText`** is extracted from the response, which is stored under the **`results`** key. The generated text is retrieved from the first result (`[0]`), specifically from the **`outputText`** field.\n",
    "\n",
    "5. **Handling Errors**:\n",
    "   - If an error occurs (such as access being denied), the **`except`** block catches the error.\n",
    "   - If the error is an **AccessDeniedException**, troubleshooting resources are printed to the console to help resolve the issue.\n",
    "   - If it’s another type of error, the error is re-raised to be handled elsewhere or logged.\n",
    "\n",
    "This code prepares the request, sends it to the model, handles potential errors, and extracts the generated text to be used in the application.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3748383a-c140-407f-a7f6-8f140ad57680",
   "metadata": {
    "id": "3748383a-c140-407f-a7f6-8f140ad57680",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Apology for Negative Feedback\n",
      "\n",
      "Dear John Doe,\n",
      "\n",
      "I am writing to express my deepest apologies for the negative feedback you provided regarding the service provided by our customer support engineer.\n",
      "\n",
      "We take customer feedback seriously and always strive to provide the best possible service. It is disheartening to hear that we have fallen short of your expectations.\n",
      "\n",
      "I would like to assure you that we are taking steps to address the issues you raised. Our customer support engineer has been thoroughly trained to handle customer interactions with care and professionalism. We are also reviewing our customer service processes to identify areas for improvement.\n",
      "\n",
      "In addition, I would like to offer you a complimentary service to make up for the negative experience you had. Please let me know how you would like to proceed.\n",
      "\n",
      "Once again, I apologize for any inconvenience you have experienced. We value your feedback and will use it to improve our services.\n",
      "\n",
      "Sincerely,\n",
      "Bob\n",
      "Customer Service Manager\n",
      "AnyCompany\n"
     ]
    }
   ],
   "source": [
    "# The relevant part of the response starts after the first newline character '\\n'.\n",
    "# Extract the email text by slicing the response string from after the first newline.\n",
    "\n",
    "email = outputText[outputText.index('\\n')+1:]  # Find the first newline and get everything after it\n",
    "print(email)  # Print the generated email\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yP4uh5lGnC-u",
   "metadata": {
    "id": "yP4uh5lGnC-u"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This section of the code extracts the **generated email** from the **AI model's response**.\n",
    "\n",
    "1. **Extracting the Relevant Text**:\n",
    "   - The **generated text** (`outputText`) includes the entire response from the model, and the **email content** starts after the first newline character (`\\n`).\n",
    "   - **`outputText.index('\\n')`** finds the index of the first **newline character** in the response.\n",
    "\n",
    "2. **Slicing the String**:\n",
    "   - **`outputText[outputText.index('\\n')+1:]`** slices the `outputText` string, starting **just after** the first newline character, effectively extracting the email content.\n",
    "\n",
    "3. **Printing the Email**:\n",
    "   - The **email text** is stored in the `email` variable and printed using `print(email)` so that you can view the generated email content.\n",
    "\n",
    "### Purpose:\n",
    "This code isolates the **email content** from the model’s response, removing the initial instructions or other text, and prints the **final email** generated by the AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d69e1a0",
   "metadata": {
    "id": "2d69e1a0"
   },
   "source": [
    "### Streaming Output Generation\n",
    "\n",
    "Bedrock also supports that the output can be streamed as it is generated by the model in form of chunks. This email is generated by invoking the model with streaming option. `invoke_model_with_response_stream` returns a `ResponseStream` which you can read from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad073290",
   "metadata": {
    "id": "ad073290",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\u001b[31m**Chunk 1**\u001b[0m\n",
      ":\n",
      "Subject: Apology for Negative Feedback\n",
      "\n",
      "Dear John Doe,\n",
      "\n",
      "I am writing to express my sincere apologies for the negative feedback you provided regarding the service provided by our customer support engineer.\n",
      "\n",
      "We take customer feedback very\n",
      "\n",
      "\t\t\u001b[31m**Chunk 2**\u001b[0m\n",
      " seriously and always strive to deliver the best possible service. It is disheartening to hear that we have fallen short of your expectations.\n",
      "\n",
      "I would like to assure you that we are taking steps to address the issues you raised. We will be conducting a thorough investigation to identify the root cause of the problem and implement necessary changes to prevent\n",
      "\n",
      "\t\t\u001b[31m**Chunk 3**\u001b[0m\n",
      " similar issues from occurring in the future.\n",
      "\n",
      "In addition, I would like to offer you a complimentary service to make up for the inconvenience you experienced. Please let me know your preferred method of compensation.\n",
      "\n",
      "Once again, I apologize for any negative experience you had with our customer support engineer and assure you that we are committed to making things right.\n",
      "\n",
      "Sincerely,\n",
      "Bob\n",
      "Customer Service Manager\n",
      "AnyCompany\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the output chunks\n",
    "output = []\n",
    "\n",
    "try:\n",
    "    # Invoke the model with a response stream to get results as chunks\n",
    "    response = bedrock_client.invoke_model_with_response_stream(body=body, modelId=modelId, accept=accept, contentType=contentType)\n",
    "    stream = response.get('body')  # Get the response stream (body)\n",
    "\n",
    "    i = 1  # Counter for chunk numbers\n",
    "    if stream:\n",
    "        # Process each chunk of data from the stream\n",
    "        for event in stream:\n",
    "            chunk = event.get('chunk')  # Get the chunk of data from the event\n",
    "            if chunk:\n",
    "                # Convert the chunk from bytes to a JSON object\n",
    "                chunk_obj = json.loads(chunk.get('bytes').decode())\n",
    "                text = chunk_obj['outputText']  # Extract the generated text (email)\n",
    "                output.append(text)  # Add the chunk to the output list\n",
    "                print(f'\\t\\t\\x1b[31m**Chunk {i}**\\x1b[0m\\n{text}\\n')  # Print the current chunk\n",
    "                i += 1  # Increment the chunk counter\n",
    "\n",
    "except botocore.exceptions.ClientError as error:\n",
    "    # Handle specific AWS access errors\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Message']}\\n\"\n",
    "              \"To troubleshoot this issue, please refer to the following resources.\\n\"\n",
    "              \"https://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\n\"\n",
    "              \"https://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "    else:\n",
    "        raise error  # Re-raise other errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a788be5",
   "metadata": {
    "id": "9a788be5"
   },
   "source": [
    "The stream with response approach helps to quickly obtain the output of the model and allows the service to complete it as you read. This assists in use cases where you request the model to generate longer pieces of text. You can later combine all the chunks generated to form the complete output and use it for your use case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xCwWSqwbnToH",
   "metadata": {
    "id": "xCwWSqwbnToH"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This code interacts with **Amazon Bedrock** using the **response stream** to process and print the model's output in **chunks**.\n",
    "\n",
    "1. **Initialize an Empty List**:\n",
    "   - **`output = []`**: Initializes an empty list to store the chunks of the generated text.\n",
    "\n",
    "2. **Invoking the Model with Response Stream**:\n",
    "   - The **`invoke_model_with_response_stream()`** method sends the **request** to the AI model and retrieves the **response stream**.\n",
    "   - The **`body`**, **`modelId`**, **`accept`**, and **`contentType`** are passed to specify the request details.\n",
    "   - **`stream = response.get('body')`** gets the body of the response, which is a stream of data chunks.\n",
    "\n",
    "3. **Processing the Stream**:\n",
    "   - **`i = 1`**: A counter to keep track of the chunks as they are processed.\n",
    "   - The `for` loop iterates over the **response stream**, and each **chunk** of data is extracted and processed one by one.\n",
    "   - **`chunk = event.get('chunk')`** retrieves the chunk from the event in the stream.\n",
    "   - **`json.loads(chunk.get('bytes').decode())`** decodes the chunk from **bytes** to a JSON object.\n",
    "   - **`text = chunk_obj['outputText']`** extracts the **generated text** (email) from the JSON object.\n",
    "   - **`output.append(text)`** stores the generated text in the `output` list.\n",
    "   - **`print(f'\\t\\t\\x1b[31m**Chunk {i}**\\x1b[0m\\n{text}\\n')`** prints the current chunk, with the chunk number (`i`) highlighted in red.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - The **`try`** block ensures the process runs, and the **`except`** block handles any **AWS access errors**.\n",
    "   - If an **AccessDeniedException** occurs, an error message with troubleshooting links is displayed.\n",
    "   - Any other errors are re-raised for further handling.\n",
    "\n",
    "### Purpose:\n",
    "This code allows **real-time processing of the AI model’s output** in **chunks**, printing each chunk of the generated text as it is received. It helps handle larger outputs efficiently and provides clear feedback for debugging or error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02d48c73",
   "metadata": {
    "id": "02d48c73",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\u001b[31m**COMPLETE OUTPUT**\u001b[0m\n",
      "\n",
      ":\n",
      "Subject: Apology for Negative Feedback\n",
      "\n",
      "Dear John Doe,\n",
      "\n",
      "I am writing to express my sincere apologies for the negative feedback you provided regarding the service provided by our customer support engineer.\n",
      "\n",
      "We take customer feedback very seriously and always strive to deliver the best possible service. It is disheartening to hear that we have fallen short of your expectations.\n",
      "\n",
      "I would like to assure you that we are taking steps to address the issues you raised. We will be conducting a thorough investigation to identify the root cause of the problem and implement necessary changes to prevent similar issues from occurring in the future.\n",
      "\n",
      "In addition, I would like to offer you a complimentary service to make up for the inconvenience you experienced. Please let me know your preferred method of compensation.\n",
      "\n",
      "Once again, I apologize for any negative experience you had with our customer support engineer and assure you that we are committed to making things right.\n",
      "\n",
      "Sincerely,\n",
      "Bob\n",
      "Customer Service Manager\n",
      "AnyCompany\n"
     ]
    }
   ],
   "source": [
    "# Combine all the chunks of text into a single string\n",
    "print('\\t\\t\\x1b[31m**COMPLETE OUTPUT**\\x1b[0m\\n')  # Print a header indicating the final output\n",
    "\n",
    "# Join all the pieces of text from the output list into one complete string\n",
    "complete_output = ''.join(output)\n",
    "\n",
    "# Print the complete generated email\n",
    "print(complete_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jDxxPza7ndaz",
   "metadata": {
    "id": "jDxxPza7ndaz"
   },
   "source": [
    "### Explanation of the Code:\n",
    "\n",
    "This section of the code combines all the chunks of text that were received in the previous step and prints the **final complete output**.\n",
    "\n",
    "1. **Printing the Header**:\n",
    "   - **`print('\\t\\t\\x1b[31m**COMPLETE OUTPUT**\\x1b[0m\\n')`**: This prints a header indicating the start of the final output.\n",
    "   - The `\\x1b[31m` makes the text **red** (for emphasis) and `\\x1b[0m` resets the color after the header.\n",
    "\n",
    "2. **Combining All Chunks into One String**:\n",
    "   - **`complete_output = ''.join(output)`**: This line combines all the **chunks** of text stored in the `output` list into one **single string**. The `join()` function concatenates each chunk in the list without any spaces in between.\n",
    "   - **`output`** is a list that contains the individual chunks of text generated by the AI, and **`complete_output`** will hold the final full response.\n",
    "\n",
    "3. **Printing the Final Output**:\n",
    "   - **`print(complete_output)`**: This prints the **final generated email**, which is now a single string combining all the chunks.\n",
    "\n",
    "### Purpose:\n",
    "This part of the code combines all the text chunks received from the model into a complete, continuous string and prints the **full generated email**. It allows you to see the final output after processing the individual chunks received from the **streaming API**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b08b3b",
   "metadata": {
    "id": "64b08b3b"
   },
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "colab": {
   "provenance": []
  },
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
