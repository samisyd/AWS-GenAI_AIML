{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OulShqPGCh3Q"
   },
   "source": [
    "# **Lab: Building Conversational AI Solutions with AWS Bedrock using Converse API**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CocInM3Duye"
   },
   "source": [
    "\n",
    "### **Introduction**\n",
    "\n",
    "Welcome to this introduction to building conversational AI with Amazon Bedrock's Converse API! The primary goal of this chapter is to provide a comprehensive introduction to Amazon Bedrock APIs for generating text. While we'll explore various use cases like summarization and code generation, our focus is on understanding the API patterns.\n",
    "\n",
    "In this notebook, you will:\n",
    "\n",
    "1. Learn the basics of the Amazon Bedrock **Invoke API**\n",
    "2. Explore the more powerful **Converse API** and it's features like multi-turn conversation, streaming, or function calling\n",
    "3. Apply these APIs across various foundation models\n",
    "4. Compare results across different state-of-the-art models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUDvvhwxCh3T"
   },
   "source": [
    "## **1. Setup**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YQ0GCIgCh3U"
   },
   "source": [
    "### **1.1 Import the required libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4tRBLwfmCh3U"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import botocore\n",
    "from IPython.display import display, Markdown\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MdiLtOcxNehG"
   },
   "source": [
    "**Output:** (No output for successful import. If run in a live environment, the code would execute silently.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyV3QZcWGPrR"
   },
   "source": [
    "### **Library Imports Explanation**\n",
    "\n",
    "In this step, we are importing specific libraries that will allow us to interact with AWS services, handle JSON data, display outputs in a Jupyter notebook, and manage timing.\n",
    "\n",
    "- **`json`**: Handles **JSON data** for parsing and generating JSON objects, commonly used in API responses from AWS.\n",
    "  \n",
    "- **`boto3`**: AWS SDK for Python, allowing interaction with **AWS services** (e.g., S3, EC2) to manage resources.\n",
    "  \n",
    "- **`botocore`**: Low-level library that **boto3** uses to manage requests, errors, and session handling with AWS.\n",
    "  \n",
    "- **`IPython.display`**: Displays **rich content** (e.g., **Markdown**) in Jupyter notebooks for better presentation.\n",
    "  \n",
    "- **`time`**: Provides time functions like **`sleep()`** to pause execution and manage time intervals.\n",
    "\n",
    "These libraries enable efficient interaction with AWS, data handling, and display management in Jupyter notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-PPacq_dCh3U"
   },
   "source": [
    "### **1.2 Initial setup for clients, global variables and helper functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Jv5pNfjlCh3V"
   },
   "outputs": [],
   "source": [
    "# Initialize a boto3 session, which allows interaction with AWS services.\n",
    "# A session is used to manage AWS credentials, configurations, and region settings.\n",
    "session = boto3.session.Session()\n",
    "\n",
    "# Get the region name of the session to configure the AWS client appropriately.\n",
    "region = session.region_name\n",
    "\n",
    "# Initialize the Bedrock client using the session's region.\n",
    "# This client will be used to interact with Amazon Bedrock's API for running foundation models.\n",
    "bedrock = boto3.client(service_name='bedrock-runtime', region_name=region)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YspmvqpGNhnn"
   },
   "source": [
    "**Output:** *(No output for successful client initialization. If run in a live environment, the code would execute silently and the Bedrock client would be ready.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "9cw0--R4Ch3V"
   },
   "outputs": [],
   "source": [
    "# Define model IDs that will be used in this module\n",
    "\n",
    "# These model IDs are required to interact with the specific foundation models using Amazon Bedrock API.\n",
    "\n",
    "MODELS = {\n",
    "\n",
    "    \"Claude 3.7 Sonnet\": \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\", # Claude 3.7 Sonnet model from Anthropic, using the model ID with versioning\n",
    "    \"Claude 3.5 Sonnet\": \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\", # Claude 3.5 Sonnet model from Anthropic with versioning\n",
    "    \"Claude 3.5 Haiku\": \"us.anthropic.claude-3-5-haiku-20241022-v1:0\", # Claude 3.5 Haiku model from Anthropic, another variation with versioning\n",
    "    \"Amazon Nova Pro\": \"us.amazon.nova-pro-v1:0\",  # Amazon Nova Pro model, a generative AI model from Amazon\n",
    "    \"Amazon Nova Micro\": \"us.amazon.nova-micro-v1:0\",  # Amazon Nova Micro model, a smaller version of Amazon Nova for resource-constrained environments\n",
    "    \"Meta Llama 3.1 70B Instruct\": \"us.meta.llama3-1-70b-instruct-v1:0\" # Meta Llama 3.1 70B Instruct model from Meta, used for instruction-following tasks\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVwYsBAlNq0f"
   },
   "source": [
    "**Output:** (No output. The MODELS dictionary is defined in memory.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gz8SnU_KKTma"
   },
   "source": [
    "This block of code helps you easily reference different AI models by their unique IDs, making it simpler to switch models or call them in subsequent parts of the code.\n",
    "\n",
    "By organizing the models in a dictionary, you can easily iterate or look up specific models as needed in your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "EUr9aLckCh3V"
   },
   "outputs": [],
   "source": [
    "# Utility function to display model responses in a more readable format\n",
    "\n",
    "def display_response(response, model_name=None):\n",
    "    # If a model name is provided, display it as a Markdown header\n",
    "    if model_name:\n",
    "        display(Markdown(f\"### Response from {model_name}\"))\n",
    "\n",
    "    # Display the model's response as Markdown content (formatted text)\n",
    "    display(Markdown(response))\n",
    "\n",
    "    # Print a separator line for better readability of the output\n",
    "    print(\"\\n\" + \"-\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ffouhrINx9N"
   },
   "source": [
    "**Output:** (No output. The `display_response` function is defined in memory.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCKtdvlmCh3V"
   },
   "source": [
    "## **2. Text Summarization with Foundation Models**\n",
    "\n",
    "Let's start by exploring how to leverage Amazon Bedrock APIs for text summarization. We'll first use the basic Invoke API, then introduce the more powerful Converse API.\n",
    "\n",
    "As an example, let's take a paragraph about Amazon Bedrock from an [AWS blog post](https://aws.amazon.com/jp/blogs/machine-learning/announcing-new-tools-for-building-with-generative-ai-on-aws/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "9HD5Ja9wCh3W"
   },
   "outputs": [],
   "source": [
    "text_to_summarize = \"\"\"\n",
    "AWS took all of that feedback from customers, and today we are excited to announce Amazon Bedrock, \\\n",
    "a new service that makes FMs from AI21 Labs, Anthropic, Stability AI, and Amazon accessible via an API. \\\n",
    "Bedrock is the easiest way for customers to build and scale generative AI-based applications using FMs, \\\n",
    "democratizing access for all builders. Bedrock will offer the ability to access a range of powerful FMs \\\n",
    "for text and images—including Amazons Titan FMs, which consist of two new LLMs we're also announcing \\\n",
    "today—through a scalable, reliable, and secure AWS managed service. With Bedrock's serverless experience, \\\n",
    "customers can easily find the right model for what they're trying to get done, get started quickly, privately \\\n",
    "customize FMs with their own data, and easily integrate and deploy them into their applications using the AWS \\\n",
    "tools and capabilities they are familiar with, without having to manage any infrastructure (including integrations \\\n",
    "with Amazon SageMaker ML features like Experiments to test different models and Pipelines to manage their FMs at scale).\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBbBItTLOFn4"
   },
   "source": [
    "**Output:** *(No output. The `text_to_summarize` variable is defined.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlWG2rWJCh3W"
   },
   "source": [
    "### **2.1 Text Summarization using the Invoke Model API**\n",
    "\n",
    "Amazon Bedrock's **Invoke Model API** serves as the most basic method for sending requests to foundation models. Since each model family has its own distinct request and response format, you'll need to craft specific JSON payloads tailored to each model.\n",
    "\n",
    "For this example, we will call Claude 3.7 Sonnet via Invoke Model API (using the `invoke_model` function of the Bedrock Runtime Client) to generate a summary of our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mTFYowU6Ch3W"
   },
   "outputs": [],
   "source": [
    "# Create the prompt for summarization\n",
    "# We are using an f-string to dynamically insert the `text_to_summarize` into the prompt for the model.\n",
    "\n",
    "prompt = f\"\"\"Please provide a summary of the following text. Do not add any information that is not mentioned in the text below.\n",
    "<text>\n",
    "{text_to_summarize}  # Insert the long text to be summarized here\n",
    "</text>\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ADKzNHEMOTmh"
   },
   "source": [
    "**Output:** *(No output. The `prompt` variable is defined.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7kvJ7022Ch3W"
   },
   "outputs": [],
   "source": [
    "# Create request body for Claude 3.7 Sonnet\n",
    "\n",
    "claude_body = json.dumps({                     # Convert the dictionary into a JSON-formatted string for the API\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\", # Specifies the version of the Claude model API being called\n",
    "    \"max_tokens\": 1000,                        # Maximum number of tokens the model is allowed to generate\n",
    "    \"temperature\": 0.5,                        # Controls creativity (0 = deterministic, 1 = creative)\n",
    "    \"top_p\": 0.9,                              # Nucleus sampling to control diversity of the output\n",
    "    \"messages\": [                              # List of messages representing the conversation history\n",
    "        {\n",
    "            \"role\": \"user\",                    # Indicates that the message is coming from the user\n",
    "            \"content\": [{\"type\": \"text\", \"text\": prompt}]  # Actual text prompt asking for summarization\n",
    "        }\n",
    "    ],\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqtaCT3NOnZ2"
   },
   "source": [
    "**Output:** *(No output. The `claude_body` JSON string is created.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SENzleRtCh3W",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Invoke Model API)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "# Summary\n",
       "\n",
       "Amazon has announced Amazon Bedrock, a new service that provides API access to foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon. Bedrock aims to democratize access to generative AI by offering a user-friendly way for customers to build and scale applications. The service includes text and image models, including Amazon's new Titan LLMs. As a serverless AWS managed service, Bedrock allows customers to find appropriate models, get started quickly, customize models with their own data, and integrate them into applications using familiar AWS tools without managing infrastructure. The service also integrates with Amazon SageMaker features like Experiments and Pipelines."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Send request to Claude 3.7 Sonnet\n",
    "\n",
    "try:\n",
    "    response = bedrock.invoke_model(                     # Call the Bedrock Invoke API to run the model\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],             # The model ID for Claude 3.7 Sonnet\n",
    "        body=claude_body,                                # JSON request body created earlier\n",
    "        accept=\"application/json\",                       # Expected response format\n",
    "        contentType=\"application/json\"                   # Content type of the request body\n",
    "    )\n",
    "    response_body = json.loads(response.get('body').read())  # Read and parse the model's JSON response\n",
    "\n",
    "    # Extract and display the response text\n",
    "    claude_summary = response_body[\"content\"][0][\"text\"]     # Extract the actual summary text from the response\n",
    "    display_response(claude_summary, \"Claude 3.7 Sonnet (Invoke Model API)\")  # Display formatted output\n",
    "\n",
    "except botocore.exceptions.ClientError as error:             # Handle AWS API errors\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':\n",
    "        # Provide helpful message for access issues\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Message']}\\\n",
    "            \\nTo troubleshoot this issue please refer to the following resources.\\\n",
    "            \\nhttps://docs.aws.amazon.com/IAM/latest/UserGuide/troubleshoot_access-denied.html\\\n",
    "            \\nhttps://docs.aws.amazon.com/bedrock/latest/userguide/security-iam.html\\x1b[0m\\n\")\n",
    "    else:\n",
    "        raise error                                          # Re-throw other unexpected errors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZTq2IhSO7pL"
   },
   "source": [
    "**Expected Output:** (Assuming a successful API call, the output would look similar to this. The actual text will vary slightly each time.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfiW19CxCh3W"
   },
   "source": [
    "### **2.2 Text Summarization using the Converse API (Recommended Approach)**\n",
    "\n",
    "While the **Invoke Model API** allows direct access to foundation models, it has several limitations:\n",
    "1. it uses different request/response formats for each model family;\n",
    "2. there is no built-in support for multi-turn conversations;\n",
    "3. it requires custom handling for different model capabilities.\n",
    "\n",
    "The **Converse API** addresses these limitations by providing a unified interface. Let's explore it on our text summarization task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "aP_i0Dt6Ch3W"
   },
   "outputs": [],
   "source": [
    "# Create a converse request with our summarization task\n",
    "\n",
    "converse_request = {\n",
    "    \"messages\": [  # List of messages representing the conversation\n",
    "        {\n",
    "            \"role\": \"user\",  # Role of the sender (user in this case)\n",
    "            \"content\": [  # Content of the message\n",
    "                {\n",
    "                    \"text\": f\"Please provide a concise summary of the following text in 2-3 sentences. Text to summarize: {text_to_summarize}\"  # The summarization prompt, including the text to summarize\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ],\n",
    "    \"inferenceConfig\": {  # Configuration settings for controlling the model’s response\n",
    "        \"temperature\": 0.4,  # Controls the randomness of the model's output (lower value for more deterministic output)\n",
    "        \"topP\": 0.9,  # Nucleus sampling for controlling diversity of the output\n",
    "        \"maxTokens\": 500  # Maximum number of tokens (words) for the summary response\n",
    "    }\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMubmoGtPD2z"
   },
   "source": [
    "**Output:** *(No output. The `converse_request` dictionary is defined.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "YSlS7idHCh3X"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Converse API)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service providing API access to foundation models (FMs) from various AI companies, including Amazon's own Titan models. Bedrock simplifies the process for customers to build and scale generative AI applications through a serverless experience, allowing them to find appropriate models, customize them with private data, and integrate them using familiar AWS tools without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call Claude 3.7 Sonnet with Converse API\n",
    "\n",
    "try:\n",
    "    # Send the conversation request to the model using the Converse API\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],  # Model ID for Claude 3.7 Sonnet from the MODELS dictionary\n",
    "        messages=converse_request[\"messages\"],  # The message list containing the user prompt\n",
    "        inferenceConfig=converse_request[\"inferenceConfig\"]  # Configuration parameters for the model (temperature, maxTokens)\n",
    "    )\n",
    "\n",
    "\n",
    "    # Extract the model's response from the JSON response body\n",
    "    claude_converse_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]  # Extracts the text from the response\n",
    "    display_response(claude_converse_response, \"Claude 3.7 Sonnet (Converse API)\")  # Display the model’s response using the display_response function\n",
    "\n",
    "\n",
    "except botocore.exceptions.ClientError as error:  # Handle any AWS API client errors\n",
    "    if error.response['Error']['Code'] == 'AccessDeniedException':  # If the error is due to access denial\n",
    "        # Print the error message in red, along with instructions for troubleshooting access issues\n",
    "        print(f\"\\x1b[41m{error.response['Error']['Code']}: {error.response['Error']['Message']}\\x1b[0m\")\n",
    "        print(\"Please ensure you have the necessary permissions for Amazon Bedrock.\")\n",
    "\n",
    "    else:\n",
    "        raise error  # For other errors, raise the exception for further investigation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72-ki4VePPom"
   },
   "source": [
    "**Expected Output:** (Assuming a successful API call, the output would look similar to this. The actual text will vary slightly but adhere to the 2-3 sentence limit.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObCTS6ApCh3X"
   },
   "source": [
    "### **2.4 Easily switch between models**\n",
    "\n",
    "One of the biggest advantages of the Converse API is the ability to easily switch between models using the exact same request format. Let's compare summaries across different foundation models by looping over the model dictionary we defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-vssNjdmCh3X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully called Claude 3.7 Sonnet (took 2.88 seconds)\n",
      "✅ Successfully called Claude 3.5 Sonnet (took 3.77 seconds)\n",
      "✅ Successfully called Claude 3.5 Haiku (took 2.18 seconds)\n",
      "✅ Successfully called Amazon Nova Pro (took 1.25 seconds)\n",
      "✅ Successfully called Amazon Nova Micro (took 0.63 seconds)\n",
      "✅ Successfully called Meta Llama 3.1 70B Instruct (took 3.49 seconds)\n"
     ]
    }
   ],
   "source": [
    "# call different models with the same converse request\n",
    "\n",
    "results = {}  # Initialize an empty dictionary to store results for each model\n",
    "\n",
    "# Loop over all models defined in the MODELS dictionary\n",
    "for model_name, model_id in MODELS.items():  # model_name is the name of the model, model_id is its identifier\n",
    "    try:\n",
    "        # Record the start time to calculate response time\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Send the converse request to the model\n",
    "        response = bedrock.converse(\n",
    "            modelId=model_id,  # The model ID to be used\n",
    "            messages=converse_request[\"messages\"],  # The messages to be sent to the model\n",
    "            inferenceConfig=converse_request[\"inferenceConfig\"] if \"inferenceConfig\" in converse_request else None  # Optional inference config\n",
    "        )\n",
    "\n",
    "        # Record the end time after receiving the response\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Extract the model's response from the API response\n",
    "        model_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "        # Calculate the response time\n",
    "        response_time = round(end_time - start_time, 2)\n",
    "\n",
    "        # Store the response and time in the results dictionary\n",
    "        results[model_name] = {\n",
    "            \"response\": model_response,  # Store the model's response\n",
    "            \"time\": response_time  # Store the time taken to get the response\n",
    "        }\n",
    "\n",
    "        # Print success message with model name and response time\n",
    "        print(f\"✅ Successfully called {model_name} (took {response_time} seconds)\")\n",
    "\n",
    "    except Exception as e:  # If an error occurs during the request\n",
    "        # Print the error message\n",
    "        print(f\"❌ Error calling {model_name}: {str(e)}\")\n",
    "\n",
    "        # Store the error message and time in the results dictionary\n",
    "        results[model_name] = {\n",
    "            \"response\": f\"Error: {str(e)}\",  # Store the error message\n",
    "            \"time\": None  # No time in case of an error\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zVeWQRZyPbit"
   },
   "source": [
    "**Expected Output:** (The output below simulates successful, time-tracked calls. Actual times will vary.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Vz-6vjoSCh3X"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Claude 3.7 Sonnet (took 2.88 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new API service that provides access to foundation models (FMs) from various AI companies, including Amazon's own Titan LLMs. The service democratizes generative AI by offering a serverless experience where customers can easily find, customize, and deploy text and image models without managing infrastructure. Bedrock integrates with existing AWS tools and allows private customization with customers' own data."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Claude 3.5 Sonnet (took 3.77 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon has announced Bedrock, a new service that provides easy API access to various foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon itself, including two new Amazon Titan language models. Bedrock aims to democratize access to generative AI by offering a serverless experience that allows customers to easily find, customize, and deploy FMs for text and image applications. The service integrates with existing AWS tools and eliminates the need for infrastructure management, making it simpler for builders to scale their AI-based applications."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Claude 3.5 Haiku (took 2.18 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service that provides easy access to foundation models from various AI companies through an API, making generative AI more accessible to developers. Bedrock offers a serverless experience that allows customers to quickly find, customize, and integrate foundation models into their applications without managing infrastructure. The service includes models from AI21 Labs, Anthropic, Stability AI, and Amazon, with the ability to work with text and image models."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Amazon Nova Pro (took 1.25 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service providing easy access to Foundation Models (FMs) from various providers like AI21 Labs, Anthropic, Stability AI, and Amazon via an API, enabling customers to build and scale generative AI applications. Bedrock offers a serverless experience for selecting, customizing, and deploying FMs, including Amazon's new Titan LLMs, with seamless integration into existing AWS tools and capabilities."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Amazon Nova Micro (took 0.63 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "AWS has launched Amazon Bedrock, a new service that provides easy access to a range of generative AI models from AI21 Labs, Anthropic, Stability AI, and Amazon via an API, enabling developers to quickly build and scale AI-based applications without managing infrastructure. Bedrock offers scalable, reliable, and secure access to powerful models for text and images, including new Amazon Titan models, and integrates seamlessly with AWS tools like Amazon SageMaker."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Meta Llama 3.1 70B Instruct (took 3.49 seconds)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Here is a concise summary of the text in 3 sentences:\n",
       "\n",
       "Amazon has announced Amazon Bedrock, a new service that provides access to foundation models (FMs) from leading AI labs via an API. Bedrock makes it easy for customers to build and scale generative AI-based applications using FMs, with a scalable, reliable, and secure AWS managed service. The service offers a serverless experience, allowing customers to easily find, customize, and deploy FMs into their applications without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display results in a formatted way\n",
    "\n",
    "for model_name, result in results.items():  # Loop through all models and their results\n",
    "    if \"Error\" not in result[\"response\"]:  # Check if there is no error in the model's response\n",
    "        # Display the model name and the time taken to process the request\n",
    "        display(Markdown(f\"### {model_name} (took {result['time']} seconds)\"))\n",
    "\n",
    "        # Display the model's response (summarization or output text)\n",
    "        display(Markdown(result[\"response\"]))\n",
    "\n",
    "        # Print a separator line for readability\n",
    "        print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLtnnSeWP1LC"
   },
   "source": [
    "**Expected Output:** (The following are representative examples of what the models might output, using the time placeholders from the previous step. The actual summary text would be the result of the prompt asking for a concise 2-3 sentence summary.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj5nodqmCh3X"
   },
   "source": [
    "### **2.5 Cross-Regional Inference in Amazon Bedrock**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4QE0P4xCh3X"
   },
   "source": [
    "Amazon Bedrock offers Cross-Regional Inference which automatically selects the optimal AWS Region within your geography to process your inference requests.\n",
    "\n",
    "Cross-Regional Inference offers higher throughput limits (up to 2x allocated quotas) and seamlessly manages traffic bursts by dynamically routing requests across multiple AWS regions, enhancing application resilience during peak demand periods without additional routing or data transfer costs.\n",
    "\n",
    "Customers can control where their inference data flows by selecting from a pre-defined set of regions, helping them comply with applicable data residency requirements and sovereignty laws. Moreover, this capability prioritizes the connected Bedrock API source region when possible, helping to minimize latency and improve responsiveness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2O_0zzEqCh3X"
   },
   "source": [
    "Let's see how easy it is to use Cross Region Inference by invoking the Claude 3.5 Sonnet model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "usvD03JGCh3X"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard response: Amazon has announced Bedrock, a new service that provides API access to foundation models (FMs) from various AI companies and Amazon itself. Bedrock aims to democratize access to generative AI by offering a user-friendly, serverless platform for building and scaling AI applications. The service allows customers to easily select, customize, and deploy FMs for text and image generation using familiar AWS tools and infrastructure, without the need to manage complex systems.\n",
      "Cross-region response: Amazon has introduced Bedrock, a new service that provides API access to various foundation models (FMs) from AI21 Labs, Anthropic, Stability AI, and Amazon itself, including two new large language models from Amazon's Titan series. Bedrock aims to democratize access to generative AI by offering a serverless, scalable, and secure platform for developers to easily integrate and deploy FMs into their applications. The service allows customers to find suitable models, customize them with private data, and leverage familiar AWS tools and features without managing infrastructure.\n"
     ]
    }
   ],
   "source": [
    "# Regular model invocation (standard region)\n",
    "\n",
    "standard_response = bedrock.converse(\n",
    "    modelId=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Standard model ID (default region)\n",
    "    messages=converse_request[\"messages\"]  # Passing the conversation history\n",
    ")\n",
    "\n",
    "# Cross-region inference (note the \"us.\" prefix)\n",
    "cris_response = bedrock.converse(\n",
    "    modelId=\"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",  # Cross-region model ID (US region)\n",
    "    messages=converse_request[\"messages\"]  # Passing the conversation history\n",
    ")\n",
    "\n",
    "# Print responses\n",
    "print(\"Standard response:\", standard_response[\"output\"][\"message\"][\"content\"][0][\"text\"])  # Print the model response from the standard region\n",
    "print(\"Cross-region response:\", cris_response[\"output\"][\"message\"][\"content\"][0][\"text\"])  # Print the model response from the cross-region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2ztpAnNP-mL"
   },
   "source": [
    "**Expected Output:** (The actual text will be very similar or identical, as the underlying model is the same, but they will be sourced from different endpoints.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhveURNLasqX"
   },
   "source": [
    "**Purpose**:\n",
    "   - This setup demonstrates how to invoke models both in the **standard region** (default AWS setup) and in a **cross-region** scenario by specifying a **regional prefix** in the model ID.\n",
    "   - The comparison between responses from both models helps **understand the impact of region-specific deployment**, such as response time and model consistency.\n",
    "\n",
    "This approach helps you explore and compare the differences between invoking models in the **default region** and using **cross-region inference**, providing insights into the behavior of models hosted in different regions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14XOifueCh3X"
   },
   "source": [
    "### **2.6 Multi-turn Conversations**\n",
    "The Converse API makes multi-turn conversations simple. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MIlczlxmCh3X"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Response from Claude 3.7 Sonnet (Multi-turn conversation)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Amazon Bedrock is AWS's new serverless API service that provides easy access to foundation models from multiple AI companies, allowing customers to find, customize, and deploy generative AI applications without managing infrastructure."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of a multi-turn conversation with Converse API\n",
    "\n",
    "multi_turn_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",  # First message from the user (initial summary request)\n",
    "        \"content\": [{\"text\": f\"Please summarize this text: {text_to_summarize}\"}]  # User asks to summarize the text\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",  # Response from the assistant (Claude model)\n",
    "        \"content\": [{\"text\": results[\"Claude 3.7 Sonnet\"][\"response\"]}]  # Assistant’s first response (summary)\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",  # Follow-up message from the user (asking for a shorter summary)\n",
    "        \"content\": [{\"text\": \"Can you make this summary even shorter, just 1 sentence?\"}]  # User asks for a more concise summary\n",
    "    }\n",
    "]\n",
    "\n",
    "try:\n",
    "    # Send the multi-turn conversation to the model using the Converse API\n",
    "    response = bedrock.converse(\n",
    "        modelId=MODELS[\"Claude 3.7 Sonnet\"],  # Specify the model (Claude 3.7 Sonnet)\n",
    "        messages=multi_turn_messages,  # Provide the conversation history (multi-turn)\n",
    "        inferenceConfig={\"temperature\": 0.2, \"maxTokens\": 500}  # Set inference configuration to control creativity and length\n",
    "    )\n",
    "\n",
    "    # Extract the model's response using the correct structure\n",
    "    follow_up_response = response[\"output\"][\"message\"][\"content\"][0][\"text\"]  # Extract the summarized response\n",
    "\n",
    "    # Display the follow-up response from the assistant\n",
    "    display_response(follow_up_response, \"Claude 3.7 Sonnet (Multi-turn conversation)\")\n",
    "\n",
    "except Exception as e:\n",
    "    # Catch and display any errors encountered during the request\n",
    "    print(f\"Error: {str(e)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUqgI40RQzhM"
   },
   "source": [
    "**Expected Output:** (The response will be a single sentence, demonstrating the model maintained context from the previous turn.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zdep8Oq8bq95"
   },
   "source": [
    "In this step, we explore how to handle **multi-turn conversations** using the Converse API, where the model generates responses based on the previous messages in the conversation.\n",
    "\n",
    "#### Key Points:\n",
    "\n",
    "1. **Multi-turn Conversation Setup**:\n",
    "   - The `multi_turn_messages` list contains multiple messages, each representing an interaction between the **user** and the **assistant** (model).\n",
    "   - **First message**: A summarization prompt from the **user** asking the model to summarize the given text.\n",
    "   - **Second message**: The **assistant's** response (model's output) to the summarization prompt.\n",
    "   - **Third message**: A follow-up question from the **user**, requesting the model to shorten the summary even further.\n",
    "\n",
    "2. **Calling the Converse API**:\n",
    "   - The `bedrock.converse()` function sends the entire **multi-turn conversation** to the model specified by the **modelId**.\n",
    "   - The **`inferenceConfig`** includes parameters like **`temperature`** (controls creativity) and **`maxTokens`** (limits response length).\n",
    "\n",
    "3. **Extracting and Displaying the Response**:\n",
    "   - After receiving the response from the API, the `follow_up_response` is extracted from the returned JSON structure.\n",
    "   - The response is then displayed using the `display_response()` function, which presents it in a user-friendly format.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - If any error occurs (e.g., network issues, model errors), an exception is caught, and an error message is printed for troubleshooting.\n",
    "   - This helps ensure smooth execution of the API calls and helps identify and resolve any issues quickly.\n",
    "\n",
    "\n",
    "This example demonstrates how to **manage multi-turn conversations** with the **Converse API**, enabling more **interactive communication** with the model. It allows the model to reference previous exchanges, providing more meaningful and contextually aware responses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lphqa3KrCh3X"
   },
   "source": [
    "### **2.7 Streaming Responses with ConverseStream API**\n",
    "\n",
    "For longer generations, you might want to receive the content as it's being generated. The ConverseStream API supports streaming, which allows you to process the response incrementally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "MsJJSpFPCh3Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Example of streaming with Converse API\n",
    "\n",
    "def stream_converse(model_id, messages, inference_config=None):\n",
    "    if inference_config is None:\n",
    "        inference_config = {}  # Set default inference config if none provided\n",
    "\n",
    "    print(\"Streaming response (chunks will appear as they are received):\\n\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    full_response = \"\"  # Initialize an empty string to store the full response\n",
    "\n",
    "    try:\n",
    "        # Sending the conversation to the model and enabling streaming\n",
    "        response = bedrock.converse_stream(\n",
    "            modelId=model_id,  # Model ID to specify which model to use\n",
    "            messages=messages,  # The conversation history to be sent to the model\n",
    "            inferenceConfig=inference_config  # Inference configuration (temperature, max tokens, etc.)\n",
    "        )\n",
    "\n",
    "        # Retrieve the stream of the response from the model\n",
    "        response_stream = response.get('stream')\n",
    "        if response_stream:\n",
    "            for event in response_stream:  # Iterate through each event in the response stream\n",
    "\n",
    "                # Check for message start event and display the role (user or assistant)\n",
    "                if 'messageStart' in event:\n",
    "                    print(f\"\\nRole: {event['messageStart']['role']}\")\n",
    "\n",
    "                # If a content block delta is present, display the text content\n",
    "                if 'contentBlockDelta' in event:\n",
    "                    print(event['contentBlockDelta']['delta']['text'], end=\"\")\n",
    "\n",
    "                # Check for message stop event and display the stop reason\n",
    "                if 'messageStop' in event:\n",
    "                    print(f\"\\nStop reason: {event['messageStop']['stopReason']}\")\n",
    "\n",
    "                # If metadata is present, display usage and latency information\n",
    "                if 'metadata' in event:\n",
    "                    metadata = event['metadata']\n",
    "                    if 'usage' in metadata:  # Display token usage information\n",
    "                        print(\"\\nToken usage\")\n",
    "                        print(f\"Input tokens: {metadata['usage']['inputTokens']}\")\n",
    "                        print(f\"Output tokens: {metadata['usage']['outputTokens']}\")\n",
    "                        print(f\"Total tokens: {metadata['usage']['totalTokens']}\")\n",
    "                    if 'metrics' in event['metadata']:  # Display latency information\n",
    "                        print(f\"Latency: {metadata['metrics']['latencyMs']} milliseconds\")\n",
    "\n",
    "            # End of the stream, display a separator\n",
    "            print(\"\\n\" + \"-\" * 80)\n",
    "\n",
    "        return full_response  # Return the full response (not updated here, as the response is printed in chunks)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors that occur during the streaming process\n",
    "        print(f\"Error in streaming: {str(e)}\")\n",
    "        return None  # Return None if an error occurs\n",
    "\n",
    "print(\"Done\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VwQF6BoBQ8LR"
   },
   "source": [
    "**Output:** *(No output. The `stream_converse` function is defined in memory.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "gO4vCo0oCh3Y"
   },
   "outputs": [],
   "source": [
    "# Let's try streaming a longer summary\n",
    "\n",
    "# Define the streaming request, which contains the user's conversation history.\n",
    "# In this case, we are asking the model to provide a detailed summary of the text provided above.\n",
    "\n",
    "streaming_request = [\n",
    "    {\n",
    "        \"role\": \"user\",  # The 'role' indicates that the user is sending the request to the model.\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": f\"\"\"Please provide a detailed summary of the following text, explaining its key points and implications:\n",
    "\n",
    "                {text_to_summarize}  # This is the actual text content that we want summarized.\n",
    "\n",
    "                Make your summary comprehensive but clear.  # Additional instructions for the model to ensure clarity and comprehensiveness.\n",
    "                \"\"\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DAwl8vu9RH8B"
   },
   "source": [
    "**Output:** *(No output. The `streaming_request` list is defined.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "pNQaWNnhCh3Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming response (chunks will appear as they are received):\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Role: assistant\n",
      "# Summary of Amazon Bedrock Announcement\n",
      "\n",
      "## Key Points\n",
      "\n",
      "Amazon has announced **Amazon Bedrock**, a new service that provides API access to foundation models (FMs) from multiple AI companies including AI21 Labs, Anthropic, Stability AI, and Amazon itself. This announcement comes in response to customer feedback seeking easier access to generative AI capabilities.\n",
      "\n",
      "The service features:\n",
      "\n",
      "- **Multiple model options** for both text and image generation\n",
      "- **Amazon's own Titan foundation models**, including two newly announced large language models (LLMs)\n",
      "- A **serverless architecture** eliminating infrastructure management needs\n",
      "- **Private customization** capabilities allowing customers to fine-tune models with their own data\n",
      "- **Integration with existing AWS tools** including SageMaker ML features like Experiments and Pipelines\n",
      "\n",
      "## Implications\n",
      "\n",
      "1. **Democratization of AI access**: Amazon is positioning Bedrock as a way to make powerful AI models accessible to all developers, not just those with specialized expertise or resources.\n",
      "\n",
      "2. **Competitive positioning**: AWS is clearly responding to the growing market for AI model APIs (like those from OpenAI) by creating a unified access point for multiple model providers.\n",
      "\n",
      "3. **Enterprise focus**: The emphasis on security, scalability, and integration with existing AWS tools suggests Bedrock is designed particularly for enterprise customers who need production-ready AI capabilities.\n",
      "\n",
      "4. **Multi-model strategy**: Rather than betting on a single AI approach, AWS is providing access to multiple foundation models, allowing customers to select the best option for their specific use case.\n",
      "\n",
      "5. **Reduced technical barriers**: The serverless approach and integration with familiar AWS tools lowers the technical threshold for implementing generative AI in applications.\n",
      "\n",
      "This announcement represents AWS's strategic move to become a central player in the generative AI ecosystem by leveraging its cloud infrastructure strengths while partnering with specialized AI companies.\n",
      "Stop reason: end_turn\n",
      "\n",
      "Token usage\n",
      "Input tokens: 299\n",
      "Output tokens: 403\n",
      "Total tokens: 702\n",
      "Latency: 9428 milliseconds\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Only run this when you're ready to see streaming output\n",
    "\n",
    "# This line of code calls the 'stream_converse' function to initiate the streaming conversation with the specified model.\n",
    "\n",
    "streamed_response = stream_converse(\n",
    "    MODELS[\"Claude 3.7 Sonnet\"],  # The model being used for this conversation, \"Claude 3.7 Sonnet\" in this case.\n",
    "    streaming_request,  # The conversation request (including the detailed prompt) defined earlier.\n",
    "    inference_config={\"temperature\": 0.4, \"maxTokens\": 1000}  # Inference parameters to control creativity and output length.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxQuQf_GRVWE"
   },
   "source": [
    "**Expected Output:** (The output would appear in real-time chunks, followed by metadata upon completion. This is a simulated, completed streaming output.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHdE0zrdCh3i"
   },
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In this notebook, we explored how to effectively interact with Amazon **Bedrock** and **Converse API** to perform tasks like **text summarization** and **real-time streaming**. Here's a summary of what we've learned:\n",
    "\n",
    "1. **Text Summarization with Invoke API**:\n",
    "   - We began by demonstrating how to summarize text using the **Invoke Model API**, where we sent a single request to the model and extracted the response.\n",
    "   - We further extended this by using the **Converse API**, which simplifies multi-turn conversations and allows us to send more complex, dynamic requests like **multi-turn conversations** and **real-time streaming**.\n",
    "\n",
    "2. **Cross-Region Requests**:\n",
    "   - We explored how to call models both in the **default region** and with **cross-region inference**, allowing us to better understand the impact of latency and model availability.\n",
    "\n",
    "3. **Streaming Responses**:\n",
    "   - We integrated **real-time streaming** of model responses using the **Converse API**, which enabled us to receive and display outputs as they are generated, making interactions more dynamic.\n",
    "\n",
    "4. **Error Handling**:\n",
    "   - Throughout the examples, we implemented robust **error handling** to catch common issues like permission errors, ensuring the code runs smoothly and provides helpful troubleshooting resources when necessary.\n",
    "\n",
    "5. **Benefits of Converse API**:\n",
    "   - The **Converse API** proved to be a powerful tool for simplifying requests and responses by standardizing the format, supporting multi-turn conversations, and offering easy configuration options for controlling model behavior (like temperature, maxTokens, etc.).\n",
    "\n",
    "\n",
    "The integration of **Amazon Bedrock**, **Converse API**, and **Claude 3.7 Sonnet** offers a streamlined way to interact with advanced foundation models, enabling real-time, high-quality text generation tasks such as summarization, multi-turn conversations, and interactive feedback, ideal for use in a wide range of applications such as chatbots, content generation, and AI-driven applications.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
